{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2) PCA Basics.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNl4lGwqGgCe"
      },
      "source": [
        "# Matrix Factorization: Principal Component Analysis (PCA)\n",
        "\n",
        "Principal Component Analysis, or PCA, might be the most popular technique for dimensionality reduction with dense data (few zero values). The scikit-learn library provides the PCA class implementation of Principal Component Analysis that can be used as a dimensionality reduction data transform. The “n_components” argument can be set to configure the number of desired dimensions in the output of the transform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E608T8G-G9GM"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMmkz1Wsf8hr"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fbdtc43tHEyE"
      },
      "source": [
        "## Load data\n",
        "\n",
        "We will use the make_classification() function to create a test binary classification dataset. The dataset will have 1,000 examples with 20 input features, 10 of which are informative and 10 of which are redundant. This provides an opportunity for each technique to identify and remove redundant input features. The fixed random seed for the pseudorandom number generator ensures we generate the same synthetic dataset each time the code runs.\n",
        "\n",
        "It is a binary classification task and we will evaluate a LogisticRegression model after each dimensionality reduction transform. The model will be evaluated using the gold standard of repeated stratified 10-fold cross-validation. The mean and standard deviation classification accuracy across all folds and repeats will be reported."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k5epfFaHDPh"
      },
      "source": [
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=10, random_state=7)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5f9Gf6vHrDr",
        "outputId": "3d1dc6f2-b865-42e2-de03-db070e9094e4"
      },
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 20)\n",
            "(1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRY8ExdRHfwZ"
      },
      "source": [
        "## Naive comparison model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cT6moibHedA"
      },
      "source": [
        "# define the model\n",
        "model = LogisticRegression()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFf9-UDNHo8P"
      },
      "source": [
        "# evaluate model\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgNR3oWLH0F_",
        "outputId": "6fae3367-be50-4a08-fe8f-1356d6d052d2"
      },
      "source": [
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.824 (0.034)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmFNyntcIG3v"
      },
      "source": [
        "##  Model with PCA\n",
        "\n",
        "We will use a Pipeline to combine the data transform and model into an atomic unit that can be evaluated using the cross-validation procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFjE7h-PH08v"
      },
      "source": [
        "# define the pipeline\n",
        "steps = [('pca', PCA(n_components=10)), ('m', LogisticRegression())]\n",
        "model = Pipeline(steps=steps)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCCTLdzlIhjW"
      },
      "source": [
        "# evaluate model\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsHaRrr9I46t",
        "outputId": "feabd43a-1116-44db-e48b-226f148a6caa"
      },
      "source": [
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.824 (0.034)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oneXkFtmJSGM"
      },
      "source": [
        "In this case, we don’t see any lift in model performance in using the PCA transform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yei0u6CCJCft"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}