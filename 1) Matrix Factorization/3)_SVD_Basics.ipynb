{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3) SVD Basics.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUVwOaX6K20B"
      },
      "source": [
        "# Singular Value Decomposition (SVD) - Basic Application \n",
        "\n",
        "Singular Value Decomposition, or SVD, is one of the most popular techniques for dimensionality reduction for sparse data (data with many zero values). The scikit-learn library provides the TruncatedSVD class implementation of Singular Value Decomposition that can be used as a dimensionality reduction data transform. The “n_components” argument can be set to configure the number of desired dimensions in the output of the transform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVFkPrpWLPtY"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh2ddijEJfA5"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZFuaW_wLYJe"
      },
      "source": [
        "## Load data\n",
        "\n",
        "We will use the make_classification() function to create a test binary classification dataset. The dataset will have 1,000 examples with 20 input features, 10 of which are informative and 10 of which are redundant. This provides an opportunity for each technique to identify and remove redundant input features. The fixed random seed for the pseudorandom number generator ensures we generate the same synthetic dataset each time the code runs.\n",
        "\n",
        "It is a binary classification task and we will evaluate a LogisticRegression model after each dimensionality reduction transform. The model will be evaluated using the gold standard of repeated stratified 10-fold cross-validation. The mean and standard deviation classification accuracy across all folds and repeats will be reported."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__0kJDW3LV05"
      },
      "source": [
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=10, random_state=7)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoTeKsqwLa2x",
        "outputId": "55f373d7-3868-4f21-c94b-d126eff62c16"
      },
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 20)\n",
            "(1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pODu7mg-LeOb"
      },
      "source": [
        "## Baseline model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHtIhA5DLcKx"
      },
      "source": [
        "# define the model\n",
        "model = LogisticRegression()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN3Oqa_oLksG"
      },
      "source": [
        "# evaluate model\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyuKCtTxLlB-",
        "outputId": "d8bff02f-31f4-4360-958f-44ad799ad5ce"
      },
      "source": [
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.824 (0.034)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-2u10poLovv"
      },
      "source": [
        "##  Model with SVD\n",
        "\n",
        "We will use a Pipeline to combine the data transform and model into an atomic unit that can be evaluated using the cross-validation procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1fd1JjsLmg8"
      },
      "source": [
        "# define the pipeline\n",
        "steps = [('svd', TruncatedSVD(n_components=10)), ('m', LogisticRegression())]\n",
        "model = Pipeline(steps=steps)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jWXhFV7LtkG"
      },
      "source": [
        "# evaluate model\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNUjXSAOLwN-",
        "outputId": "2626a17b-a18d-4c6f-ead4-7831e257a9ce"
      },
      "source": [
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.824 (0.034)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgP4kWuYL0mR"
      },
      "source": [
        "In this case, we don’t see any lift in model performance in using the SVD transform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnVin6c2Lx8e"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}
