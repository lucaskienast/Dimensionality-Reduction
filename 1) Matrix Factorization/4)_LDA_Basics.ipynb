{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4) LDA Basics.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_uK64OKMAoH"
      },
      "source": [
        "# Matrix Factorization: Linear Discriminant Analysis (LDA)\n",
        "\n",
        "Linear Discriminant Analysis, or LDA, is a multi-class classification algorithm that can be used for dimensionality reduction. The number of dimensions for the projection is limited to 1 and C-1, where C is the number of classes. In this case, our dataset is a binary classification problem (two classes), limiting the number of dimensions to 1. The scikit-learn library provides the LinearDiscriminantAnalysis class implementation of Linear Discriminant Analysis that can be used as a dimensionality reduction data transform. The “n_components” argument can be set to configure the number of desired dimensions in the output of the transform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgFButd6MXIm"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dRwtOn3L-J7"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtKLrEm6McbA"
      },
      "source": [
        "## Load data\n",
        "\n",
        "We will use the make_classification() function to create a test binary classification dataset. The dataset will have 1,000 examples with 20 input features, 10 of which are informative and 10 of which are redundant. This provides an opportunity for each technique to identify and remove redundant input features. The fixed random seed for the pseudorandom number generator ensures we generate the same synthetic dataset each time the code runs.\n",
        "\n",
        "It is a binary classification task and we will evaluate a LogisticRegression model after each dimensionality reduction transform. The model will be evaluated using the gold standard of repeated stratified 10-fold cross-validation. The mean and standard deviation classification accuracy across all folds and repeats will be reported."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbfTBEC5MaqI"
      },
      "source": [
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=10, random_state=7)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HA51VYOuMehn",
        "outputId": "c1d6b9d5-8f03-4bef-ff81-c82dc9a0f474"
      },
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 20)\n",
            "(1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyJsUl56Mieb"
      },
      "source": [
        "## Baseline model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k0PaQXUMg9G"
      },
      "source": [
        "# define the model\n",
        "model = LogisticRegression()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM-RcZiCMj7Z"
      },
      "source": [
        "# evaluate model\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CElTOFNMMrJn",
        "outputId": "5cd32568-651a-47c9-d4bf-527ff12584b7"
      },
      "source": [
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.824 (0.034)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6il5WPBHM2DA"
      },
      "source": [
        "##  Model with LDA\n",
        "\n",
        "We will use a Pipeline to combine the data transform and model into an atomic unit that can be evaluated using the cross-validation procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M05BXhNCMsc2"
      },
      "source": [
        "# define the pipeline\n",
        "steps = [('lda', LinearDiscriminantAnalysis(n_components=1)), ('m', LogisticRegression())]\n",
        "model = Pipeline(steps=steps)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvCjTzxuNDLj"
      },
      "source": [
        "# evaluate model\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaoVSb66NEhJ",
        "outputId": "7fd2ccab-45b5-4c5c-f3b5-ee6515f3aef3"
      },
      "source": [
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.825 (0.034)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJTU0jaoNIdA"
      },
      "source": [
        "In this case, we can see a slight lift in performance as compared to the baseline fit on the raw data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYFSb2jWNF0w"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}
